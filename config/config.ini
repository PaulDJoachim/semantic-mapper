# Divergent Inference Architecture Configuration

[model]
# Model settings

model_name = Qwen/Qwen3-1.7B

# device to run model on - options are 'cpu' or 'cuda'
device = cuda


[generation]
# Stem Generation parameters

seed = 42
# token depth at which new nodes will stop being generated
max_depth = 16

# max token length of each stem
stem_length = 8

# range of stem tokens to search for pruning location: 1=full stem, .5=last half, 0=no pruning
# after generation, each stem's prune_range is searched for the largest token entropy jump and subsequent tokens are removed
# TODO change to integer
prune_range = 0.5

# initial number of stems to generate and attempt to cluster at each node
num_stems = 512

# max possible stems at a node - if no valid clusters are found with num_stems samples,
# additional stems will be generated up to max_stems_per_node
max_stems_per_node = 512

# number of stems to generate per forward pass - larger batch sizes may be faster, but will also consume more memory
batch_size = 128

# Token-level filtering
# increasing temperature creates more diverse stems, and may require a greater num_stems
# value to generate a sufficient amount for effective cluster formation
temperature = 0.8
top_k = 40
top_p = 0.9

[embeddings]
# General embedding parameters

# Embedding model for semantic analysis
model_name = sentence-transformers/paraphrase-mpnet-base-v2

# number of embeddings to generate per forward pass
batch_size = 128

# Normalized embeddings generally make clustering easier, but may reduce nuance in semantic representation
# When false, be wary of using larger prune_range values, as the embeddings become more sensitive to variations in stem length
normalize = true

# custom pooling method for embeddings - options are 'mean', 'max', 'lasttoken'
custom_pooling = mean

[clustering]
# Semantic clustering parameters

# clustering algorithm to use - options are 'hierarchical', 'optics', and 'dbscan'
cluster_type = hierarchical

# min_sample_ratio is the smallest allowable ratio of a cluster's size,
# in proportion to the total samples generated for a node. It functions as a fallback for
# cases where clustering produces many small clusters
min_sample_ratio = 0.1


[hi-clustering]
# hierarchical clustering parameters

# linkage criterion to use - options are 'average', 'complete', 'single', 'ward', 'weighted'
# TODO elaborate on these somewhere
linkage_criterion = average

# less distance creates more clusters with greater semantic similarity and fewer stems
cut_distance = 0.4

# hierarchical clusters are sorted by their sample count and pulled from the top until
# the total number of pulled samples is at least cluster_top_p * total_samples
# This is to prevent the algorithm from returning hundreds of small clusters
# TODO: consider implementing this as a UI slider that dynamically redraws the tree
#       we're already generating the clusters; might as well preserve the option to look at them
cluster_top_p = 0.9

# force return the largest cluster even if it doesn't meet the min_sample_ratio
# nodes with 0 clusters will simply terminate
force_cluster = false

# Use adaptive distance threshold based on percentile of distances (overrides cut_distance)
use_adaptive_distance = true
variance_scaling_factor = 1.0

[OPTICS-clustering]
# OPTICS clustering parameters
xi = 0.2

[DBSCAN-clustering]
# DBSCAN clustering parameters

# smaller epsilon values require samples to be semantically closer to form a cluster
eps = 0.20

[visualization]
# Visualization settings

template_path = templates/tree_template.html
output_dir = ./output/renders

# Type of PCA normalization to apply to positions after PCA reduction
# current options are 'unit_sphere' or 'none'
pca_normalization = none


[analysis]
# Analysis report parameters
output_dir = ./output/reports
