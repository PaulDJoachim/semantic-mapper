# Divergent Inference Architecture Configuration

[model]
# Model settings
model_name = gpt2-xl
;model_name = microsoft/Phi-3.5-mini-instruct
device = cuda
pad_token_mode = eos

[generation]
# Stem Generation parameters

seed = 42
# max token depth of tree
max_depth = 48
# length of each stem
stem_length = 12
# range of stem tokens to search for pruning location: 1=full stem, .5=last half, etc.
prune_range = .4
# min number of stems to generate at each node
num_stems = 128
# max possible stems at a node
max_stems_per_node = 512
# number of stems to generate per forward pass
batch_size = 16

# Token-level filtering
temperature = 0.82
top_k = 0
top_p = 0.75

[embeddings]
# Embedding model for semantic analysis
model_name = all-MiniLM-L6-v2
batch_size = 32

[clustering]
# Semantic clustering parameters
cluster_type = hierarchical
# percentage of total generated node stems required to form a cluster
# if a cluster doesn't contain min_sample_ratio of the total generated node stems,
# it is discarded and its stems will be considered noise
min_sample_ratio = 0.12

[hi-clustering]
# hierarchical clustering parameters
linkage_criterion = complete
# less distance creates more clusters with greater semantic similarity and fewer stems
cut_distance = 0.65
# force returning the largest cluster even if it doesn't meet the min_sample_ratio
# nodes with 0 clusters will simply terminate
force_cluster = false

[DBSCAN-clustering]
# DBSCAN clustering parameters
# smaller values require samples to be semantically closer to form a cluster
# larger values allow samples to be semantically further apart to form a cluster
eps = 0.3

[visualization]
# Visualization settings
template_path = templates/tree_template.html
output_dir = ./output/renders
compress_linear = true
max_display_depth = 2000

[analysis]
# Analysis parameters
output_dir = ./output/reports
min_path_depth = 5
sample_paths_count = 10
progress_interval = 100
