# Divergent Inference Architecture Configuration

[model]
# Model settings
model_name = gpt2-xl
device = cuda
pad_token_mode = eos

[generation]
# Stem Generation parameters

seed = 42
# max token depth of tree
max_depth =  25
# length of each stem
stem_length = 5
# min number of stems to generate at each node
num_stems = 50
# max possible stems at a node
max_stems_per_node = 10000
# number of stems to generate per forward pass
batch_size = 50

# Token-level filtering
temperature = 0.88
top_k = 0
top_p = 0.9

[embeddings]
# Embedding model for semantic analysis
model_name = all-MiniLM-L6-v2
batch_size = 128

[clustering]
# Semantic clustering parameters

# distance threshold for DBSCAN clustering -
# smaller values lead to more branching, with less semantic divergence
# larger values lead to less branching, but with more significant semantic divergence
# large values risk missing significant semantic divergence
# small values risk over-branching and branches with trivial divergence
eps = 0.20
# percentage of total generated node stems to form a cluster
min_sample_ratio = 0.15
# number of clusters required to trigger branching
min_clusters = 2

[visualization]
# Visualization settings
template_path = templates/tree_template.html
output_dir = ./output
compress_linear = true
max_display_depth = 2000

[analysis]
# Analysis parameters
min_path_depth = 5
sample_paths_count = 10
progress_interval = 100
